{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Subset the Pre-Computed Travel Cost Matricies\n",
    "To calculate spatial access measures, data on travel times or distances between origins and destinations must be used. If you only need distances between origins and destinations, the `access` package will calculate Euclidean distances for your projected data. If you need travel times for a specific travel mode (walking, public transit, or driving) you need to generate these so-called travel time (or travel cost) matrices from other sources. We provide pre-calculated travel cost matricies between blocks and tracts [here](https://access.readthedocs.io/en/latest/resources.html) for the 20 largest cities in the US and for the entire country.\n",
    "\n",
    "Each dataset are point-to-point distance matrices generated by the Center for Spatial Data Science. The matrices come as bzipped CSVs. Cities are listed according to their containing county code. The origins for each distance matrix are the population-weighted centroids that lie within each county, and the destinations are any population-weighted centroids that lie within 100 km of the buffered county.\n",
    "\n",
    "In this notebook, we show how to download the national travel cost matrix and subset it for your area of interest. In the example below, we will subset the matrix to include only the tracts in Cook County, IL. By the end of this notebook, you should be able to:\n",
    "- [Download the travel matrix](#Downloading-the-National-Travel-Matrix)\n",
    "- [Define your area of interest using the Census GEOID](#Defining-Area-of-Interest-with-Census-GEOID)\n",
    "- [Use dask to subset the travel cost matrix to your area of interest](#Use-Dask-to-Subset-the-Travel-Cost-Matrix)\n",
    "- [Use the terminal and basic bash commands to subset the travel cost matrix to your area of interest](#Use-Bash-Commands-to-Subset-the-Travel-Cost-Matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the National Travel Matrix\n",
    "You can find the national driving travel cost matrix on the [`access` read the docs page](https://access.readthedocs.io/en/latest/resources.html). You can download the dataset either by clicking on the link as shown in the image below, or as shown in the code which follows.\n",
    "<img src=\"../docs/_static/images/screenshot_cost_website.png\" style=\"width: 750px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://uchicago.box.com/shared/static/prapz7ac7vwuz44nnab3dhe10vbg55cz.bz2 \\\n",
    "    -O ./national_cost_matrix.csv.bz2  # noqa: E501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see the file in the current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Area of Interest with Census GEOID\n",
    "You can find more information on the Census GEOID [here](https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html). For our example, we will filter the dataset for Cook County, IL. The State ID for Illinois is `17` and the County ID for Cook County is `031`.TOgether, we want to filter our dataset for rows that have an origin and destination that start with `17031`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Dask to Subset the Travel Cost Matrix\n",
    "Since the uncompressed csv is too large to load into the memory of computers with less than 8GB of memory, we cannot load the data in its entirety into memory using `pandas`. Even if you can load it into memory, using `pandas` may take too long. Instead, we will show you how to use [`dask`](https://docs.dask.org/en/latest/delayed.html) to subset the travel cost matrix. First, we must uncompress the file and extract the csv. Note: you must install the command line package `bzip2` if it's not already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! bzip2 -dk national_cost_matrix.csv.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a big file, uncompressed -- more than 4GB.  You may want to delete it when you're done.\n",
    "\n",
    "Next, we'll read in the csv dataset using dask dataframe's `.read_csv()` method. Note: nothing has yet happend by running this command. Dask uses lazy evaluation, so nothing will compute until you use the `.compute()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_cost_matrix = dd.read_csv(\"./national_cost_matrix.csv\")\n",
    "\n",
    "# Filter out faulty rows which exist in the data - will be cleaned and replaced soon.\n",
    "national_cost_matrix = national_cost_matrix[national_cost_matrix[\"origin\"] != \"origin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now convert the origin and destination columns to type `str` and use the newly converted string column to filter for `17031`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_cost_matrix[\"origin\"] = national_cost_matrix.origin.astype(str)\n",
    "national_cost_matrix[\"destination\"] = national_cost_matrix.destination.astype(str)\n",
    "\n",
    "cook_county_fips = \"17031\"\n",
    "\n",
    "cook_cost_matrix = national_cost_matrix[\n",
    "    national_cost_matrix[\"origin\"].str.startswith(cook_county_fips)\n",
    "    & national_cost_matrix[\"destination\"].str.startswith(cook_county_fips)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our delayed transformations setup, now we can execute our data transformations and have `dask` complete those transformations in parallel with the `.compute()` method. **Warning: you might need at least 8GB of memory to successfully execute this process. If you have 8GB of memory and it fails, try closing unused programs and try again.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cook_cost_matrix = cook_cost_matrix.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cook_cost_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to save a copy of the subsetted data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cook_cost_matrix.to_csv(\"cook_county_cost_matrix.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Bash Commands to Subset the Travel Cost Matrix\n",
    "You can also use a bash script to subset the national Travel Cost matrix provided you have two files, each containing the origin and destination geoids you're interested in capturing.\n",
    "\n",
    "Here's an example where we use two files containing a small list of origin and destination locations, split into separate `origin.csv` and `destinations.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dest = pd.DataFrame(\n",
    "    {\n",
    "        \"origin\": [\"17031010100\", \"17031010101\", \"17031010102\"],\n",
    "        \"destination\": [\"17031820605\", \"17031292400\", \"17031827100\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dest[\"origin\"].to_csv(\"./origins.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17031010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17031010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17031010102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0  17031010100\n",
       "1  17031010101\n",
       "2  17031010102"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./origins.csv\", header=None).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dest[\"destination\"].to_csv(\"./destinations.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17031820605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17031292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17031827100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0  17031820605\n",
       "1  17031292400\n",
       "2  17031827100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./destinations.csv\", header=None).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the two files above, you'll also need the `r.awk` script (included in this directory) and the unzipped national Travel Cost matrix.\n",
    "\n",
    "Below is the simple `r.awk` script.  What this does, line by line, is:\n",
    "\n",
    "0. Keep track of the file number, by checking if it's the first line of a file.\n",
    "1. For file number 1, add the first field (we've done `-F','`, so this is comma separated) to a list of origins.\n",
    "2. For file number 2, add the first field to a list of destinations.\n",
    "3. For other files (i.e. number 3), check to see if field one is an origin and field two is a destination.  If so, it will print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "FNR == 1 { file+=1 }\r\n",
      "\r\n",
      "file == 1 { ORIGINS[$1]=1 ; next } \r\n",
      "\r\n",
      "file == 2 { DESTINATIONS[$1]=1 ; next } \r\n",
      "\r\n",
      "($1 in ORIGINS) && ($2 in DESTINATIONS)\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat r.awk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell shows, how to use the script to create your desired subset. The command follows the structure:\n",
    "\n",
    "`awk -F',' -f ./r.awk [origins_file] [destinations_file] [travel_cost_matrix] > [subset_output]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!awk -F',' -f ./r.awk ./origins.csv ./destinations.csv ./national_cost_matrix.csv \\\n",
    "    > subset_cost_matrix.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read the newly created travel matrix subset, making sure to include column names for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_matrix = pd.read_csv(\n",
    "    \"./subset_cost_matrix.csv\", names=[\"origin\", \"destination\", \"minutes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17031010100</td>\n",
       "      <td>17031292400</td>\n",
       "      <td>46.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17031010100</td>\n",
       "      <td>17031820605</td>\n",
       "      <td>59.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17031010100</td>\n",
       "      <td>17031827100</td>\n",
       "      <td>62.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        origin  destination  minutes\n",
       "0  17031010100  17031292400    46.88\n",
       "1  17031010100  17031820605    59.28\n",
       "2  17031010100  17031827100    62.75"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
